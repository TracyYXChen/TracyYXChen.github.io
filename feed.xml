<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://tracyyxchen.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://tracyyxchen.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-10-07T21:01:08+00:00</updated><id>https://tracyyxchen.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">DocDancer: authoring ultra-responsive documents with layout generation</title><link href="https://tracyyxchen.github.io/blog/2023/docdancer/" rel="alternate" type="text/html" title="DocDancer: authoring ultra-responsive documents with layout generation" /><published>2023-10-07T00:00:00+00:00</published><updated>2023-10-07T00:00:00+00:00</updated><id>https://tracyyxchen.github.io/blog/2023/docdancer</id><content type="html" xml:base="https://tracyyxchen.github.io/blog/2023/docdancer/"><![CDATA[<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vRod9jlCYSbBdbMx_EipC6ZyZqSpH26lLi7fyUWA0dnbZkTL7dL82DY_9M7CfPOggS1p3Ty3gGD57bb/embed?start=false&amp;loop=false&amp;delayms=3000" frameborder="0" width="960" height="569" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>]]></content><author><name></name></author><category term="human-computer-interaction" /><summary type="html"><![CDATA[slides for VL/HCC 2023]]></summary></entry><entry><title type="html">ä¸­å­¦æ¯•ä¸šåå¹´å</title><link href="https://tracyyxchen.github.io/blog/2023/decade/" rel="alternate" type="text/html" title="ä¸­å­¦æ¯•ä¸šåå¹´å" /><published>2023-08-05T00:00:00+00:00</published><updated>2023-08-05T00:00:00+00:00</updated><id>https://tracyyxchen.github.io/blog/2023/decade</id><content type="html" xml:base="https://tracyyxchen.github.io/blog/2023/decade/"><![CDATA[<p>ä»¿ä½›å¸Œè…Šç¥è¯ä¸­ä¸æ–­æ›´æ¢éƒ¨ä»¶çš„å¿’ä¿®æ–¯ä¹‹èˆ¹ï¼ˆthe ship of theseusï¼‰ï¼Œåœ¨è¿‡å»åå¹´çš„èˆªç¨‹é‡Œï¼Œæˆ‘æœ‰æ˜¾è‘—æ”¹å˜ï¼š</p>

<p>2013å¹´ï¼Œæˆ‘ä»ä¸€æ‰€ä¸­å›½è¥¿éƒ¨çš„é«˜ä¸­æ¯•ä¸šï¼Œæ—¥å¸¸ç”¨è¯­æ˜¯å››å·è¯ã€‚æˆ‘çš„ä¹¦å’Œç¬”è®°éƒ½æ˜¯çº¸è´¨çš„ï¼Œåˆšåˆšæ‹¥æœ‰äº†äººç”Ÿä¸­ç¬¬ä¸€å°ç¬”è®°æœ¬ç”µè„‘å’Œæ™ºèƒ½æ‰‹æœºï¼Œä¸ä¼šç¼–ç¨‹ï¼Œé‡åˆ°é—®é¢˜ç™¾åº¦ä¸€ä¸‹ã€‚</p>

<p>2023å¹´ï¼Œæˆ‘åœ¨ç¾å›½ä¸œéƒ¨è¯»è®¡ç®—æœºåšå£«ï¼ˆè¿˜æ²¡æœ‰æ¯•ä¸šï¼‰ï¼Œæ—¥å¸¸ç”¨è¯­æ˜¯è‹±è¯­å¤¹æ‚æ™®é€šè¯ã€‚æˆ‘å‡ ä¹ä¸å†ä¹°çº¸è´¨ä¹¦ï¼Œä¹Ÿå¾ˆä¹…æ²¡æœ‰åœ¨çº¸ä¸Šå†™å­—ã€‚æˆ‘æœ‰ä¸¤ä¸ªKindleï¼Œä¸€ä¸ªçœ‹ä¹¦ï¼Œå¦ä¸€ä¸ªçœ‹PDFæ–‡ä»¶ï¼Œæˆ‘çš„ç¬”è®°æ•£è½åœ¨äº‘ç«¯ï¼šGoogle Driveå’ŒNotion. é™¤äº†ç¬”è®°æœ¬ç”µè„‘å’Œæ™ºèƒ½æ‰‹æœºï¼Œæˆ‘è¿˜æœ‰äº†å¹³æ¿ï¼Œæ™ºèƒ½æ‰‹è¡¨å’Œä¸€ä¸ªå°˜å°å¤šå¹´çš„Oculus VR headset. æˆ‘æ¯å¤©ä¸»è¦çš„å·¥ä½œå†…å®¹æ˜¯ç¼–ç¨‹ï¼Œé‡åˆ°é—®é¢˜å°±é—®ChatGPT.</p>

<p>æˆ‘ä¹Ÿè®¸ä¸å†æ˜¯å½“åˆé‚£è‰˜èˆ¹ï¼Œå› ä¸ºåœ¨è¿™æ®µèˆªç¨‹é‡Œï¼Œæˆ‘é‡æ–°å®šä¹‰äº†ï¼š</p>

<ul>
  <li>ä¸ºä»€ä¹ˆï¼ˆWhyï¼‰ï¼šä»å¢åŠ ä¸ªäººé€‰æ‹©æƒï¼ˆoption valueï¼‰åˆ°ä½¿å‘½å¯¼å‘ï¼ˆmission-drivenï¼‰</li>
  <li>åšä»€ä¹ˆï¼ˆWhatï¼‰ï¼šä»ä¼˜åŒ–ï¼ˆoptimizationï¼‰åˆ°é€†å‘ï¼ˆcontrarianï¼‰</li>
  <li>æ€ä¹ˆåšï¼ˆHowï¼‰ï¼šä»å›ºå®šæ€ç»´æ¨¡å¼ï¼ˆfixed mindsetï¼‰æˆé•¿æ€ç»´æ¨¡å¼ï¼ˆgrowth mindsetï¼‰</li>
</ul>

<h3 id="1-ä¸ºä»€ä¹ˆwhyä»å¢åŠ ä¸ªäººçš„é€‰æ‹©æƒoption-valueåˆ°ä½¿å‘½å¯¼å‘mission-driven">1. ä¸ºä»€ä¹ˆï¼ˆWhyï¼‰ï¼šä»å¢åŠ ä¸ªäººçš„é€‰æ‹©æƒï¼ˆoption valueï¼‰åˆ°ä½¿å‘½å¯¼å‘ï¼ˆmission-drivenï¼‰</h3>

<p>åå¹´å‰çš„æˆ‘è¿˜å¸¦ç€åº”è¯•æ•™è‚²ç•™ä¸‹çš„ç—•è¿¹ã€‚åº”è¯•æ•™è‚²å¼ºè°ƒä¼˜è´¨èµ„æºçš„ç¨€ç¼ºï¼Œéœ€è¦ååˆ—å‰èŒ…æ‰èƒ½è·å¾—æ›´å¤šé€‰æ‹©æƒã€‚è™½ç„¶è¿›å…¥å¤§å­¦ä¹‹åï¼Œä¼˜è´¨èµ„æºä¸å†ç¨€ç¼ºï¼šå¯¹ä¸€åæ™®é€šç§‘å¤§å­¦ç”Ÿè€Œè¨€ï¼Œç»§ç»­æ·±é€ å’Œæ‰¾ä¸€ä»½å¥½å·¥ä½œå¹¶ä¸éš¾ã€‚ç„¶è€Œï¼Œâ€œè·å¾—æ›´å¤šé€‰æ‹©æƒâ€ä»¿ä½›ä¸‰ä½“é‡Œçš„æ€æƒ³é’¢å°ï¼Œä¾ç„¶æŒ‡å¯¼ç€æˆ‘çš„å†³ç­–ï¼š</p>

<ul>
  <li>å‡ºå›½æ·±é€ ï¼šè®©æœªæ¥å¯é€‰çš„å·¥ä½œåœ°ç‚¹ä»ä¸€ä¸ªå›½å®¶æ‰©å¤§åˆ°ä¸¤ä¸ªå›½å®¶ã€‚</li>
  <li>ä»åŸå­ï¼ˆææ–™åŒ–å­¦ï¼‰åˆ°æ¯”ç‰¹ï¼ˆè®¡ç®—æœºï¼‰ï¼šä¸å†å—å®éªŒè®¾å¤‡çš„åœ°ç†ä½ç½®é™åˆ¶ï¼Œå¯ä»¥è¿œç¨‹å·¥ä½œ</li>
</ul>

<p>å¦‚ä»Šçš„æˆ‘ï¼Œçš„ç¡®æ¯”åå¹´å‰æ‹¥æœ‰äº†æ›´å¤šçš„é€‰æ‹©æƒã€‚ç„¶è€Œæˆ‘æ„è¯†åˆ°å¦‚æœæ²¿ç€è¿™æ¡è·¯èµ°ä¸‹å»ï¼Œæˆ‘ä¼šæˆä¸ºæ ‡å‡†çš„é›‡ä½£å†›ï¼ˆmercenaryï¼‰ï¼Œå“ªé‡Œé’±å¤šå»å“ªé‡Œã€‚åœ¨ç°ä»£ç¤¾ä¼šï¼Œæ¸©é¥±å¹¶ä¸æ˜¯é—®é¢˜ï¼Œæˆ‘åº”è¯¥é—®è‡ªå·±ï¼Œæˆ‘çš„ä½¿å‘½ï¼ˆmissionï¼‰æ˜¯ä»€ä¹ˆï¼Ÿ</p>

<p>é€šè¿‡é˜…è¯»å†å²ï¼Œæˆ‘é€æ¸å‘ç°äº†è‡ªå·±çš„ä½¿å‘½ï¼Œé‚£å°±æ˜¯<strong>æ¢ç´¢å¥³æ€§å¯ä»¥åˆ›é€ çš„ç¤¾ä¼šä»·å€¼çš„ä¸Šé™</strong>ã€‚</p>

<p>åœ¨æµ©å¦‚çƒŸæµ·çš„äººç±»å†å²ä¸­ï¼Œå¥³æ€§åˆ›é€ çš„ç¤¾ä¼šä»·å€¼å¾€å¾€å–å†³äºåˆå§‹å€¼ï¼šæ˜¯å¦å‡ºèº«åé—¨æˆ–è€…å€¾å›½å€¾åŸã€‚ä½œä¸ºä¸€åæ™®é€šå¥³æ€§ï¼Œå‡ºç”Ÿåœ¨ç‹¬ç”Ÿå­å¥³æ”¿ç­–ä¸‹çš„ä¸­å›½ï¼Œåˆæ¥åˆ°DEI (Diversity, Equity and Inclusion) è¿åŠ¨å¦‚ç«å¦‚è¼çš„ç¾å›½ï¼Œæ˜¯ä½•å…¶å¹¸è¿ï¼šæˆ‘æ—¢æ²¡æœ‰ä½“éªŒè¿‡ä¸­å›½å‡ åƒå¹´ä»¥æ¥é‡ç”·è½»å¥³çš„ä¼ ç»Ÿï¼Œä¹Ÿæ²¡æœ‰ä½“éªŒè¿‡ç¾å›½ä¸Šä¸–çºªå¥³æ€§æ¯•ä¸šç”Ÿé¢ä¸´çš„æ¶åŠ£çš„èŒåœºç¯å¢ƒï¼ˆæ¯”å¦‚åæ¥è¢«èª‰ä¸ºé™†åœ°å«æ˜Ÿä¹‹æ¯çš„Virginia Norwoodï¼Œ1947å¹´ä»éº»çœç†å·¥å­¦é™¢æ•°å­¦ç‰©ç†ä¸“ä¸šæœ¬ç§‘æ¯•ä¸šä¹‹åï¼Œæ²¡æœ‰ä¸€é—´ä¸“ä¸šå¯¹å£çš„å…¬å¸ç»™å¥¹offerï¼Œç¬¬ä¸€ä»½å·¥ä½œæ˜¯å»ç™¾è´§å•†åº—å–å¥³è£…ï¼‰ã€‚å…¶æ¬¡ï¼Œä»ä¸ªäººæˆé•¿ç¯å¢ƒæ¥çœ‹ï¼Œæˆ‘ä¹Ÿæ˜¯å¹¸è¿çš„ã€‚æˆ‘å‡ºç”Ÿåœ¨ä¸€ä¸ªé‡è§†æ•™è‚²çš„å®¶åº­ï¼Œé‡åˆ°çš„åŒå­¦ä»¬éƒ½èªæ˜å‹å–„ï¼Œå¯¼å¸ˆä»¬éƒ½ç»™äºˆæˆ‘å¾ˆå¤šæ”¯æŒã€‚åœ¨21ä¸–çºªï¼Œç²¾é€šä¸­è‹±æ–‡å’Œç¼–ç¨‹ï¼Œè¿™ä¹Ÿè®¸æ˜¯å†å²ä¸Šå‰æ‰€æœªæœ‰çš„æ’åˆ—ç»„åˆï¼š<strong>å¥³æ€§ä¸ªäººèƒ½åŠ›å’Œç¤¾ä¼šéœ€æ±‚çš„é«˜åº¦åŒ¹é…ï¼Œè®©åˆ›é€ æ›´å¤šçš„ç¤¾ä¼šä»·å€¼æˆä¸ºå¯èƒ½ã€‚</strong></p>

<p>æˆ‘æ›¾ç»åªå¸Œæœ›è¿‡ä¸Šä¼˜æ¸¥èˆ’é€‚çš„ç”Ÿæ´»ï¼Œä½†æ˜¯ç°åœ¨æˆ‘æ„è¯†åˆ°ï¼Œå¦‚æœ21ä¸–çºªçš„æˆ‘ä¸å»æ¢ç´¢å¥³æ€§å¯ä»¥åˆ›é€ çš„ç¤¾ä¼šä»·å€¼çš„ä¸Šé™ï¼Œé‚£å°±æ˜¯è¾œè´Ÿäº†å†å²ï¼Œå°±åƒ16ä¸–çºªçš„è‘¡è„ç‰™æ°´æ‰‹ä¸å»è¿œæ´‹èˆªæµ·ã€‚</p>

<p>ä¸ºäº†æ¢ç´¢è¿™ä¸ªå˜é‡çš„ä¸Šé™ï¼Œæˆ‘åº”è¯¥æ‰¾åˆ°ä¸€ä¸ª<strong>æˆ‘æ‰€æ“…é•¿çš„ï¼Œæ‰€çƒ­çˆ±çš„å’Œæœ€å¤§åŒ–åˆ›é€ ç¤¾ä¼šä»·å€¼çš„äº¤é›†</strong>ã€‚ç›®å‰çœ‹æ¥ï¼Œè¿™ä¸ªäº¤é›†æ˜¯æˆç«‹æˆ–è€…åŠ å…¥ä¸€å®¶ç§‘æŠ€startupï¼šæˆ‘æ“…é•¿ç¼–ç¨‹ï¼Œçƒ­çˆ±å¼€å‘æ–°äº§å“ï¼šä½œä¸ºäººæœºäº¤äº’çš„åšå£«ç”Ÿï¼Œæˆ‘çš„æ¯ä¸ªè¯¾é¢˜éƒ½æ˜¯ä¸€ä¸ªäº§å“åŸå‹ï¼šä»ç•Œé¢è®¾è®¡ï¼Œè½¯ä»¶å¼€å‘åˆ°ç”¨æˆ·ç ”ç©¶ï¼›è€Œè¿‡å»å‡ åå¹´ï¼Œç§‘æŠ€å…¬å¸åˆ›é€ çš„ç¤¾ä¼šä»·å€¼æœ‰ç›®å…±ç¹ã€‚</p>

<p>å°½ç®¡ä»æ•°å­¦æœŸæœ›ä¸Šæ¥è¯´ï¼Œè¿™ä¸æ˜¯ä¸€æ¡æ˜æ™ºçš„é“è·¯ï¼Œåˆ›ä¸šä¹æ­»ä¸€ç”Ÿï¼Œæˆ‘æœªæ¥çš„æ”¶å…¥å’Œå·¥ä½œ/ç”Ÿæ´»å¹³è¡¡æ˜¾ç„¶ä¸å¦‚æˆä¸ºå¤§å…¬å¸çš„ç¬¬åä¸‡ä¸ªå‘˜å·¥ã€‚ä½†æ˜¯è¿™æ¡è·¯é€šå‘æ›´é«˜çš„ä¸Šé™ï¼Œæˆ‘ä¹Ÿå”¯æœ‰æ²¿ç€è¿™æ¡è·¯èµ°ä¸‹å»ï¼Œæ‰ä¼šåœ¨å›é¦–ä¸€ç”Ÿçš„æ—¶å€™ï¼Œè®©æˆ‘è§‰å¾—æ²¡æœ‰è¾œè´Ÿå†å²ï¼Œnot history, but herstory.</p>

<h3 id="2-åšä»€ä¹ˆwhatä»ä¼˜åŒ–optimizationåˆ°é€†å‘contrarian">2. åšä»€ä¹ˆï¼ˆWhatï¼‰ï¼šä»ä¼˜åŒ–ï¼ˆoptimizationï¼‰åˆ°é€†å‘(contrarianï¼‰</h3>

<p>åå¹´å‰çš„æˆ‘é¢å¯¹çš„é—®é¢˜æ˜¯å¦‚ä½•æé«˜è€ƒè¯•æˆç»©ï¼Œè¿™å±äºä¼˜åŒ–é—®é¢˜ã€‚æŠŠæ—¶é—´ç²¾åŠ›æŠ•èµ„åœ¨ä¼˜åŒ–é—®é¢˜ä¸Šæœ‰å¾ˆå¤šå¥½å¤„ï¼šæœ‰å¯è¡Œçš„æ–¹æ¡ˆï¼Œæ¸…æ™°çš„æ—¶é—´è¡¨å’Œç¨³å®šçš„å›æŠ¥ã€‚æˆç»©å¥½å°±èƒ½ä¸Šåæ ¡ï¼Œå¸®å…¬å¸ä¼˜åŒ–ç°æœ‰äº§å“å°±èƒ½å‡èŒåŠ è–ªã€‚</p>

<p>ç„¶è€Œä¼˜åŒ–é—®é¢˜æ‰€åˆ›é€ ä»·å€¼çš„æ•°é‡çº§æ˜¯æœ‰é™çš„ï¼Œæƒ³è¦æé«˜æ•°é‡çº§éœ€è¦é€†å‘æ€ç»´ï¼ˆcontrarianï¼‰ï¼Œæ€è€ƒé‚£äº›çœ‹èµ·æ¥ä¸é‚£ä¹ˆåˆç†çš„äº‹æƒ…å’ŒèƒŒåçš„é™åˆ¶æ¡ä»¶ï¼ˆconstraintsï¼‰ï¼Œæ¯”å¦‚è¯´ï¼Œ</p>

<ul>
  <li>Airbnbï¼šæœ‰äººæ„¿æ„å‡ºç§Ÿå®¶é‡Œçš„ç©ºæˆ¿é—´å—ï¼Ÿæœ‰äººæ„¿æ„ç¡åœ¨é™Œç”Ÿäººçš„æˆ¿é—´é‡Œå—ï¼Ÿ</li>
  <li>Figma: æµè§ˆå™¨èƒ½æ”¯æŒå¤æ‚è½¯ä»¶å—ï¼Ÿè®¾è®¡å¸ˆæ„¿æ„å’Œåˆ«äººåˆä½œç¼–è¾‘åŒä¸€ä¸ªè®¾è®¡æ–‡æ¡£å—ï¼Ÿ</li>
</ul>

<p>ä»ä¸åˆç†å˜æˆåˆç†æ„å‘³ç€é™åˆ¶æ¡ä»¶çš„æ¶ˆå¤±ï¼Œæ¯”å¦‚è¿œç¨‹å·¥ä½œè®©æ•°å­—æ¸¸ç‰§æˆä¸ºå¯èƒ½ã€‚æœ‰çš„é™åˆ¶æ¡ä»¶æ¯”åˆ«çš„æ›´å¼ºï¼Œæ¯”å¦‚æ¶‰åŠç¡¬ä»¶å’Œç”Ÿç‰©ï¼šæ‰“é€ ä¼šé£çš„æ±½è½¦æ¯”æ‰“é€ Twitteræ›´éš¾ï¼Œç ”å‘æ–°è¯æ¯”ç ”å‘åˆ†å­åŠ¨åŠ›å­¦æ¨¡æ‹Ÿè½¯ä»¶æ›´éš¾ï¼›æ¯”å¦‚æ¶‰åŠåˆ°ç›‘ç®¡ï¼šåŒ»ç–—ä¿é™©çš„åˆ›æ–°ã€‚</p>

<p>æ€è€ƒé™åˆ¶æ¡ä»¶æ¯”æ€è€ƒä¼˜åŒ–é—®é¢˜æ›´éš¾ï¼Œä½†æ˜¯å¦‚æœæˆåŠŸï¼Œåˆ›é€ ç¤¾ä¼šä»·å€¼çš„æ•°é‡çº§è¿œé«˜äºä¼˜åŒ–é—®é¢˜ï¼Œæ˜¯å€¼å¾—æŒç»­æ€è€ƒçš„æ–¹å‘ã€‚</p>

<h3 id="3-æ€ä¹ˆåš-howä»å›ºå®šæ€ç»´æ¨¡å¼fixed-mindsetæˆé•¿æ€ç»´æ¨¡å¼growth-mindset">3. æ€ä¹ˆåš ï¼ˆHowï¼‰ï¼šä»å›ºå®šæ€ç»´æ¨¡å¼ï¼ˆfixed mindsetï¼‰æˆé•¿æ€ç»´æ¨¡å¼ï¼ˆgrowth mindsetï¼‰</h3>

<p>åå¹´å‰çš„æˆ‘å±äºå›ºå®šæ€ç»´æ¨¡å¼ï¼Œæˆ‘ç¾¡æ…•é‚£äº›çœ‹èµ·æ¥èªæ˜çš„åŒå­¦ï¼Œä¹Ÿå®³æ€•è‡ªå·±æ˜¾å¾—æ„šè ¢è€Œä¸æ•¢é—®é—®é¢˜ã€‚åœ¨å¤§å­¦é‡Œä¹Ÿä¸æ•¢é€‰å¤ªéš¾çš„è¯¾ï¼Œå› ä¸ºè§‰å¾—å’Œåˆ«çš„åŒå­¦ç›¸æ¯”è‡ªå·±å¤©èµ‹ä¸å¤Ÿã€‚</p>

<p>æˆ‘åæ¥åˆ‡æ¢åˆ°äº†æˆé•¿å‹çš„æ€ç»´æ¨¡å¼ï¼šäººä¸–é—´çš„å¤§éƒ¨åˆ†äº‹æƒ…ï¼Œå°¤å…¶æ˜¯å·¥ç¨‹é—®é¢˜ï¼Œæ˜¯æ™®é€šäººå¯ä»¥é€šè¿‡å­¦ä¹ è€Œç²¾é€šçš„ï¼Œå¹¶ä¸”å­¦ä¹ èƒ½åŠ›æœ¬èº«ä¹Ÿå¯ä»¥ä¸æ–­æé«˜ã€‚æˆ‘åæ¥çš„ç»å†ä¹Ÿè¯å®äº†è¿™ä¸€ç‚¹ï¼šæˆ‘21å²æ‰å†™ä¸‹ç¬¬ä¸€è¡ŒPythonï¼Œä½†ç»è¿‡ä¸æ–­åŠªåŠ›å­¦ä¹ ï¼Œæˆ‘çš„ç¼–ç¨‹èƒ½åŠ›å¯ä»¥èƒœä»»è®¡ç®—æœºç³»çš„ç ”ç©¶ç”Ÿè¯¾ç¨‹å’Œç ”ç©¶è¯¾é¢˜ã€‚æˆ‘ä¹Ÿå¼€å§‹ç§¯æçš„åœ¨åˆ«äººçš„æŠ¥å‘Šä¸Šé—®é—®é¢˜ã€‚æœ‰çš„é—®é¢˜ç¡®å®æ„šè ¢ï¼Œä½†æœ‰çš„é—®é¢˜ä¸é‚£ä¹ˆæ„šè ¢ï¼Œåè€Œè¿˜å¼€å¯äº†å¾ˆå¥½çš„è®¨è®ºï¼Œè®©æˆ‘å—ç›ŠåŒªæµ…ã€‚</p>

<p>æ¥ä¸‹æ¥ï¼Œæˆ‘éœ€è¦æ›´å¤šçš„å­¦ä¹ ï¼šäº†è§£æœ€æ–°çš„ç§‘æŠ€è¿›å±•å’Œç§‘æŠ€äº§å“çš„å†å²ï¼Œå’Œåˆ«äººè®¨è®ºæƒ³æ³•æ”¶é›†åé¦ˆã€‚å¹¸è¿çš„æ˜¯ï¼Œä½œä¸ºäººæœºäº¤äº’çš„åšå£«ç”Ÿï¼Œæˆ‘çš„æ¯ä¸€ä¸ªè¯¾é¢˜å…¶å®éƒ½åƒä¸€ä¸ªstartup ideaï¼Œä¹Ÿå¯ä»¥æ²¿ç€ç›®å‰çš„è¯¾é¢˜æ€è€ƒå•†ä¸šåŒ–çš„å¯èƒ½ã€‚</p>

<p>ä¸‹ä¸€ä¸ªåå¹´ï¼Œå¸Œæœ›èƒ½å¤Ÿè¢«è‡ªå·±æ‰€åˆ›é€ çš„ç¤¾ä¼šä»·å€¼èƒŒä¹¦ï¼ˆendorsementï¼‰ï¼Œè€Œä¸å†æ˜¯æ•™è‚²èƒŒæ™¯å’Œå…¬å¸ç»å†ã€‚</p>]]></content><author><name></name></author><category term="general" /><summary type="html"><![CDATA[ä»¿ä½›å¸Œè…Šç¥è¯ä¸­ä¸æ–­æ›´æ¢éƒ¨ä»¶çš„å¿’ä¿®æ–¯ä¹‹èˆ¹ï¼ˆthe ship of theseusï¼‰ï¼Œåœ¨è¿‡å»åå¹´çš„èˆªç¨‹é‡Œï¼Œæˆ‘æœ‰æ˜¾è‘—æ”¹å˜ï¼š]]></summary></entry><entry><title type="html">about/è‡ªæˆ‘ä»‹ç»</title><link href="https://tracyyxchen.github.io/blog/2023/about/" rel="alternate" type="text/html" title="about/è‡ªæˆ‘ä»‹ç»" /><published>2023-07-20T00:00:00+00:00</published><updated>2023-07-20T00:00:00+00:00</updated><id>https://tracyyxchen.github.io/blog/2023/about</id><content type="html" xml:base="https://tracyyxchen.github.io/blog/2023/about/"><![CDATA[<p>ä¸­æ–‡ç‰ˆåœ¨æœ€ä¸‹æ–¹</p>

<h2 id="about-me">About me</h2>

<p>Hi, Iâ€™m Yuexi (Tracy) Chen, a computer science Ph.D. student working on human-AI interaction <a href="https://hcil.umd.edu/" target="_blank" rel="noopener">at the University of Maryland</a>. I developed AI-assisted tools for designers/creators (some projects collaborated with Adobe). In the future, I plan to either join a startup or start my own company.</p>

<p>I come from <a href="https://en.wikipedia.org/wiki/Neijiang" target="_blank" rel="noopener">Neijiang</a>, a city on the bank of a tributary of the Yangtze River in Sichuan Province, China. If youâ€™ve read <a href="https://en.wikipedia.org/wiki/River_Town:_Two_Years_on_the_Yangtze" target="_blank" rel="noopener">River Town by Peter Hessler</a>, my life is like a sequel to the book: my mom attended a teachersâ€™ college and later became an English teacher. I received my B.S. from the <a href="https://en.wikipedia.org/wiki/University_of_Science_and_Technology_of_China" target="_blank" rel="noopener">University of Science and Technology of China (USTC)</a>, a university in <a href="https://en.wikipedia.org/wiki/Hefei" target="_blank" rel="noopener">Hefei</a>, Anhui Province; hence I consider Hefei my second hometown.</p>

<p>When I am not coding, I am usually playing tennis. I was on the varsity team when I was an undergrad and participated in collegiate tennis competitions (10% competitive as NCAA Division I, as we typically train for 2-4 hours a week). I currently play in the womenâ€™s and mixed doubles leagues of the United States Tennis Association as an <a href="https://www.usta.com/content/dam/usta/pdfs/10013_experience_player_ntrp_guidelines.pdf" target="_blank" rel="noopener">NTRP 3.5 player</a>. Inspired by the movie <em>Moneyball</em>, I also conduct <a href="https://github.com/TracyYXChen/tennis-data" target="_blank" rel="noopener">tennis data analytics</a>. Here is <a href="https://www.google.com/maps/d/viewer?mid=1GRiYfEKGLojztDRf_SbnKBUKRliBxOM&amp;hl=en&amp;usp=sharing">a map</a> of the tennis courts Iâ€™ve visited on the earth.</p>

<h2 id="blog-content">Blog Content</h2>

<ul>
  <li><strong>Machine Learning/Data Science</strong>: I use <a href="https://medium.com/@tracyyxchen" target="_blank" rel="noopener">Medium</a> to document my projects and lectures. However, as Medium limits the number of blogs that non-paying users can access, I decide to mirror the content on my personal website.</li>
  <li><strong>Personal Reflections (in Chinese)</strong>: Though Iâ€™m a native Chinese speaker, I started learning about machine learning and data science after coming to the United States, so I am not familiar with the related Chinese terminologies. Instead, I plan to write personal reflections in Chinese.</li>
</ul>

<h2 id="planned-updates">Planned Updates</h2>

<ul>
  <li>Technical Blogs (every 2-3 months)</li>
  <li>Chinese Blogs (every 3-4 months)</li>
  <li>In Total: 7-10 articles per year.</li>
</ul>

<h2 id="è‡ªæˆ‘ä»‹ç»">è‡ªæˆ‘ä»‹ç»</h2>
<p>æˆ‘å«é™ˆç¥è¥¿ï¼Œç®€ç§°Tracyï¼Œç›®å‰åœ¨ç¾å›½é©¬é‡Œå…°å¤§å­¦è®¡ç®—æœºç³»è¯»åšå£«ï¼Œæ–¹å‘æ˜¯<a href="https://hcil.umd.edu/" target="_blank" rel="noopener">äººæœºäº¤äº’</a>ã€‚æˆ‘åšå£«è¯¾é¢˜çš„å…¶ä¸­ä¸€éƒ¨åˆ†æ˜¯æ¢ç´¢ç»™è®¾è®¡å¸ˆå’Œåˆ›ä½œè€…ä½¿ç”¨çš„æ–°å‹å·¥å…·ï¼ˆå’ŒAdobeå…¬å¸çš„åˆä½œé¡¹ç›®ï¼‰ã€‚æˆ‘æ‰“ç®—ä»ŠååŠ å…¥åˆ›ä¸šå…¬å¸æˆ–è€…è‡ªå·±åˆ›ä¸šã€‚</p>

<p>æˆ‘æ¥è‡ªå†…æ±Ÿï¼Œä¸€åº§å››å·ä¸­å—éƒ¨çš„äº”çº¿åŸå¸‚ï¼Œæœ‰ä¸€æ¡é•¿æ±Ÿçš„æ”¯æµç©¿åŸè€Œè¿‡ã€‚å¦‚æœä½ è¯»è¿‡<a href="https://m.douban.com/book/subject/7060185/" target="_blank" rel="noopener">å½¼å¾—æµ·æ–¯å‹’çš„ã€Šæ±ŸåŸã€‹</a>ï¼Œé‚£ä¹ˆæˆ‘çš„äººç”Ÿä»¿ä½›æ˜¯å®ƒçš„ç»­é›†ï¼šåƒé‚£äº›æ¶ªé™µå¸ˆèŒƒçš„å­¦ç”Ÿä¸€æ ·ï¼Œæˆ‘å¦ˆå¦ˆå¸ˆèŒƒæ¯•ä¸šåä¹Ÿæˆä¸ºäº†ä¸€åè‹±è¯­è€å¸ˆã€‚æˆ‘æœ¬ç§‘æ¯•ä¸šäºä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦ï¼ˆ1314ï¼‰ï¼Œåœ¨åˆè‚¥ç•™ä¸‹äº†ä¸€äº›éš¾å¿˜çš„å›å¿†ï¼Œå› æ­¤æˆ‘ä¹Ÿè®¤ä¸ºåˆè‚¥æ˜¯æˆ‘çš„ç¬¬äºŒæ•…ä¹¡ã€‚</p>

<p>å½“æˆ‘ä¸å†™ä»£ç çš„æ—¶å€™ï¼Œæˆ‘å¾€å¾€åœ¨æ‰“ç½‘çƒã€‚æˆ‘ä»å°æ‰“ä¹’ä¹“çƒï¼Œä¸Šå¤§å­¦ä¹‹åæ”¹ç»ƒç½‘çƒï¼Œåæ¥åŠ å…¥äº†æ ¡é˜Ÿï¼Œæ›¾ç»ä»£è¡¨ç§‘å¤§å‚åŠ å¤§å­¦ç”Ÿç½‘çƒæ¯”èµ›ï¼ˆç”²ç»„ï¼Œéç‰¹é•¿ç”Ÿï¼‰ã€‚æˆ‘ç›®å‰ä½œä¸ºNTRP3.5é€‰æ‰‹å‚åŠ ç¾å›½ç½‘çƒåä¼šçš„å¥³å­å’Œæ··åŒè”èµ›ã€‚æˆ‘å–œæ¬¢ä¸€éƒ¨å«<a href="https://movie.douban.com/subject/3023164/">ç‚¹çƒæˆé‡‘</a>çš„ç”µå½±ï¼Œå› æ­¤æˆ‘å¶å°”ä¹Ÿåš<a href="https://github.com/TracyYXChen/tennis-data" target="_blank" rel="noopener">ç½‘çƒæ•°æ®åˆ†æ</a>ã€‚æˆ‘æœ‰ä¸€å¼ <a href="https://www.google.com/maps/d/viewer?mid=1GRiYfEKGLojztDRf_SbnKBUKRliBxOM&amp;hl=en&amp;usp=sharing" target="_blank" rel="noopener">è°·æ­Œåœ°å›¾</a>è®°å½•åœ°çƒä¸Šæˆ‘å»è¿‡çš„ç½‘çƒåœºã€‚</p>

<h2 id="åšå®¢å†…å®¹">åšå®¢å†…å®¹</h2>
<ul>
  <li>æœºå™¨å­¦ä¹ /æ•°æ®ç§‘å­¦ï¼ˆè‹±æ–‡ï¼‰ï¼šæˆ‘ä¸€ç›´ç”¨<a href="https://medium.com/@tracyyxchen" target="_blank" rel="noopener">Medium</a>æ¥è®°å½•å°å‹é¡¹ç›®å’Œè®²åº§ã€‚ç„¶è€ŒMediumé™åˆ¶éä»˜è´¹ç”¨æˆ·å¯ä»¥è®¿é—®çš„åšå®¢æ•°é‡ï¼Œäºæ˜¯æˆ‘å†³å®šåœ¨ä¸ªäººç½‘ç«™ä¸ŠåŒæ­¥ç›¸å…³å†…å®¹ã€‚</li>
  <li>ä¸ªäººæ„Ÿæ‚Ÿï¼ˆä¸­æ–‡ï¼‰ï¼šæˆ‘ä¸æ‰“ç®—å†™ä¸­æ–‡æŠ€æœ¯åšå®¢ï¼Œå› ä¸ºæˆ‘æ¥ç¾å›½ä¹‹åæ‰å¼€å§‹äº†è§£æœºå™¨å­¦ä¹ å’Œæ•°æ®ç§‘å­¦ï¼Œä¸æ¸…æ¥šè®¸å¤šæœ¯è¯­çš„ä¸­æ–‡è¡¨è¿°ã€‚</li>
</ul>

<h2 id="è®¡åˆ’æ›´æ–°">è®¡åˆ’æ›´æ–°</h2>
<ul>
  <li>æŠ€æœ¯åšå®¢ï¼ˆæ¯ä¸¤åˆ°ä¸‰ä¸ªæœˆï¼‰</li>
  <li>ä¸­æ–‡åšå®¢ï¼ˆæ¯ä¸‰åˆ°å››ä¸ªæœˆï¼‰</li>
  <li>æ€»å…±ï¼šæ¯å¹´7-10ç¯‡</li>
</ul>]]></content><author><name></name></author><category term="general" /><summary type="html"><![CDATA[ä¸­æ–‡ç‰ˆåœ¨æœ€ä¸‹æ–¹]]></summary></entry><entry><title type="html">Unlock the power of AI on your laptop - an introduction to Hugging Face</title><link href="https://tracyyxchen.github.io/blog/2023/huggingface/" rel="alternate" type="text/html" title="Unlock the power of AI on your laptop - an introduction to Hugging Face" /><published>2023-06-06T00:00:00+00:00</published><updated>2023-06-06T00:00:00+00:00</updated><id>https://tracyyxchen.github.io/blog/2023/huggingface</id><content type="html" xml:base="https://tracyyxchen.github.io/blog/2023/huggingface/"><![CDATA[<p>During the <a href="https://hcil.umd.edu/hcil-symposium-2023/" target="_blank" rel="noopener">40th HCIL Symposium</a>, I gave a tutorial on Hugging Face, a powerful machine learning (ML) platform that offers user-friendly APIs. In addition, incorporating UMD themes made the experience more enjoyable, which served as a fun backdrop.</p>

<p>As someone who has used Hugging Face for my research, I was impressed by its user-friendliness and the scope of its ML APIs. The tutorial targeted HCI/UX researchers and hobbyists with a limited computing budget but are interested in exploring new features with potential ML capabilities or in making inferences with pre-trained models.</p>

<p><strong>Outline:</strong></p>
<ul>
  <li>HuggingChat</li>
  <li>Spaces: online ML applications</li>
  <li>Transformers, a Python package</li>
</ul>

<h2 id="1-huggingchat">1. HuggingChat</h2>

<p>HuggingChat is a chatbot built on open-source models (currently, itâ€™s built on OpenAssistant LLaMa 30B SFT 6). I also compared its output with closed-source models (OpenAIâ€™s GPT-4 and Googleâ€™s Bard)</p>

<p><strong>Prompt:</strong> <em>suggest some themed lunch table ideas for an HCI symposium</em></p>

<p><strong>HuggingChat:</strong></p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/hugging-face/fig1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/hugging-face/fig1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/hugging-face/fig1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/hugging-face/fig1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>I found those named ideas delightful.</p>

<p><strong>GPT-4:</strong></p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/hugging-face/fig2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/hugging-face/fig2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/hugging-face/fig2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/hugging-face/fig2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>GPT-4 suggests broad discussion topics along with table decorations. However, it had its quirks â€” point 5, â€œmake sure the table is accessible to everyone,â€ seemed more in line with on-screen table designs rather than physical tables!</p>

<p><strong>Google Bard:</strong></p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/hugging-face/fig3-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/hugging-face/fig3-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/hugging-face/fig3-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/hugging-face/fig3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>Iâ€™m surprised that Bard does not return any discussion points but focuses on decorations and food exclusively, offering suggestions right down to specifics like grilled cheese sandwiches ğŸ˜…</p>

<p><strong>Humans</strong></p>

<p>On the human side of things, the HCIL symposium has three themed lunch tables this year:</p>

<ul>
  <li>Robotics Education</li>
  <li>Teams/creativity</li>
  <li>LLMs and the future of AI as a writing assistant</li>
</ul>

<p>It seems LLMs are too humble to suggest themselves as lunch table ideas! ğŸ¤·â€â™€</p>

<h2 id="2-hugging-face-spaces">2. Hugging Face Spaces</h2>
<p>Next, we looked at Spaces, online demos of machine-learning applications.</p>

<h3 id="object-detection-space-link">Object detection (<a href="https://huggingface.co/spaces/eddie5389/Object-Detection-With-DETR-and-YOLOS" target="_blank" rel="noopener">space link</a>)</h3>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/hugging-face/fig6-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/hugging-face/fig6-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/hugging-face/fig6-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/hugging-face/fig6.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>For instance, I demonstrated an object detection Space by uploading a photo taken at the <a href="https://dining.umd.edu/yahentamitsi-virtual-tour" target="_blank" rel="noopener">Yahentamitsi dining hall</a>. The model could identify objects within the photo, such as a backpack, pizza, and persons. (Note that it canâ€™t return labels that are not in the training dataset, here is a <a href="https://gist.github.com/AruniRC/7b3dadd004da04c80198557db5da4bda" target="_blank" rel="noopener">list of supported labels</a> in this Space)</p>

<h3 id="clip-interrogator-space-link">CLIP Interrogator (<a href="https://huggingface.co/spaces/pharma/CLIP-Interrogator" target="_blank" rel="noopener">space link</a>)</h3>
<p>Another intriguing Space is the CLIP Interrogator, which generates potential text prompts based on an image. Letâ€™s see what text prompts will be suggested for a photo of our <a href="https://goo.gl/maps/khGiycsZTLCFon4T8" target="_blank" rel="noopener">Testudo</a> during final exam weeksğŸ˜</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/hugging-face/fig7-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/hugging-face/fig7-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/hugging-face/fig7-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/hugging-face/fig7.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>If I use the output verbatim as input for Stable Diffusion, one of the generated images is: ğŸ˜†</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/hugging-face/fig4-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/hugging-face/fig4-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/hugging-face/fig4-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/hugging-face/fig4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h2 id="3-transformers-a-python-package">3. Transformers, a Python package</h2>
<p>Hugging Face offers a variety of Python packages, including Transformers, which can be used for numerous tasks, such as Natural Language Processing (NLP), Computer Vision (CV), and audio processing. I demonstrated how to use it for sentiment analysis, language modeling, zero-shot object detection, etc.</p>

<p>GitHub Repo: <a href="https://github.com/TracyYXChen/huggingFaceTutorial" target="_blank" rel="noopener">huggingFaceTutorial</a></p>

<p>First, download transformers:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install transformers
</code></pre></div></div>
<p>Import transformers:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from transformers import pipeline
</code></pre></div></div>
<h3 id="sentiment-analysis">Sentiment Analysis</h3>
<p>In sentiment analysis, the model predicts the emotional tone of the text: positive, negative, or neutral.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sentence = "Weâ€™ve done a lot since the lab launched in 1983 and weâ€™re looking forward to what faculty and students contribute in the next 40 years"
classifier = pipeline(task="sentiment-analysis")
preds = classifier(sentence)
preds = [{"score": round(pred["score"], 4), "label": pred["label"]} for pred in preds]
print(preds)
</code></pre></div></div>

<p>output:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[{'score': 0.9998, 'label': 'POSITIVE'}]
</code></pre></div></div>

<h3 id="language-modeling">Language Modeling</h3>
<p>Language modeling predicts masked words in sentences.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>text = "2023 Commencement Honors the â€˜Terrapin Gritâ€™ of &lt;mask&gt; of UMD Graduates"
fill_mask = pipeline(task="fill-mask")
preds = fill_mask(text, top_k=3)
</code></pre></div></div>

<p>I asked the audience to guess the masked word, but no one had guessed it right. The ground truth is â€œthousands,â€ which is also the top-1 prediction returned by the transformer.</p>

<h3 id="zero-shot-object-detection">Zero-shot Object Detection</h3>
<p>Unlike traditional object detectors (e.g., the detector I showed before), zero-shot object detectors can predict labels not found in the training dataset, allowing us to identify custom labels without training a model on a new dataset.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import skimage
import numpy as np
from PIL import Image, ImageDraw

# load check-point
checkpoint = "google/owlvit-base-patch32"
detector = pipeline(model=checkpoint, task="zero-shot-object-detection")

# load image
imgPath = "./symposiumPoster.jpeg"
image = Image.open(imgPath).resize((600, 300))
image = Image.fromarray(np.uint8(image)).convert("RGB")

# provide candidate labels
candidate_labels = ["poster", "yellow sofa", "green chair", "blue table cloth", "yellow table cloth"]

predictions = detector(
    image,
    candidate_labels=candidate_labels,
)

# visualize predictions
draw = ImageDraw.Draw(image)

for prediction in predictions:
    box = prediction["box"]
    label = prediction["label"]
    score = prediction["score"]
    xmin, ymin, xmax, ymax = box.values()
    draw.rectangle((xmin, ymin, xmax, ymax), outline="red", width=1)
    draw.text((xmin, ymin), f"{label}: {round(score,2)}", fill="white")
</code></pre></div></div>

<p>output:</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/hugging-face/fig5-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/hugging-face/fig5-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/hugging-face/fig5-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/hugging-face/fig5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h3 id="discussions">Discussions</h3>
<p>The tutorial ended with participants sharing their planned use cases for Hugging Face. Some exciting applications included food image analysis, survey response text analysis, hate speech classification, and image segmentation. I look forward to seeing more people unlock the potential of ML on their laptops!</p>]]></content><author><name></name></author><category term="machine-learning" /><summary type="html"><![CDATA[a short tutorial given at the HCIL symposium 2023]]></summary></entry><entry><title type="html">State-of-the-â€œartâ€ - machine learning for graphics and UI designs</title><link href="https://tracyyxchen.github.io/blog/2022/sota/" rel="alternate" type="text/html" title="State-of-the-â€œartâ€ - machine learning for graphics and UI designs" /><published>2022-08-27T00:00:00+00:00</published><updated>2022-08-27T00:00:00+00:00</updated><id>https://tracyyxchen.github.io/blog/2022/sota</id><content type="html" xml:base="https://tracyyxchen.github.io/blog/2022/sota/"><![CDATA[<p>Before starting my summer internship at Adobe Research last year, I read several papers about machine learning for design. While none of my intern projects focused on this, I found the research fascinating, so at the <a href="https://hcil.umd.edu/2022-symposium/" target="_blank" rel="noopener">HCIL symposium 2022</a>, I gave a 1-hour talk on the research progress of ML for graphics and UI design. Inspired by one of the attendees â€” <a href="https://twitter.com/haldaume3/status/1529811573942132738?cxt=HHwWhIDR4a3h_boqAAAA" target="_blank" rel="noopener">Prof. Hal DaumÃ© IIIâ€™s Tweetstorm</a> â€” I wrote this post to share a skimmable version of the talk.</p>

<p><strong>Talk format</strong>:</p>

<p>I focus on two topics: <strong>â€œfind design examples/inspirationsâ€</strong> and <strong>â€œgenerate layout automaticallyâ€</strong> via an informal literature review of selected papers published at top-tier machine learning/human-computer interaction conferences/journals,</p>

<p>The symposium talk was hybrid: ~40 attendees joined the talk, and half joined virtually; some were designers or students in design-related majors, and some are ML/HCI researchers. Thanks to <a href="https://www.slido.com/" target="_blank" rel="noopener">Slido</a>, I collected primary data about audiencesâ€™ preferences on features enabled by machine learning/data-driven methods.</p>

<p><strong>Table of contents</strong>:</p>

<p>Part 1: Find design examples/inspirations</p>

<ul>
  <li>by Cascading Style Sheets (CSS) properties</li>
  <li>by descriptive words</li>
  <li>by visual properties</li>
  <li>by images</li>
  <li>by a chatbot</li>
</ul>

<p>Part 2: Generate layout automatically</p>

<ul>
  <li>by constraints</li>
  <li>by â€œenergiesâ€ (design guidelines)</li>
  <li>by keywords/categories</li>
  <li>by spatial relations with background and embellishments</li>
</ul>

<h2 id="part-1-find-design-examplesinspirations">Part 1: Find design examples/inspirations</h2>

<p><strong>Motivation</strong>: Designers routinely seek inspiration. Findings from <a href="https://arxiv.org/pdf/2102.05216.pdf" target="_blank" rel="noopener">a study with 24 UI/UX designers</a> found that most searched keywords on Google, Behance, Pinterest, etc., and collected search results in different aspects: layout variations (N=22), font styles (N=14), color palettes (N=11).</p>

<p><strong>Pain points</strong>:</p>
<ul>
  <li>Itâ€™s difficult to find the right textual description to search for</li>
  <li>Itâ€™s time-consuming to find good examples</li>
  <li>Search results do not meet the design requirement</li>
</ul>

<p>How can machine learning/data-driven methods help?</p>

<h3 id="11-search-by-cascading-style-sheets-css-properties">1.1 search by Cascading Style Sheets (CSS) properties</h3>
<p>Cascading Style Sheets (CSS) is a style sheet language decorating web content. Users can choose any part on a webpage and inspect information about font family/size, color, padding, etc. (Read about how to do it.)</p>

<p><a href="http://vis.stanford.edu/files/2013-Webzeitgeist-CHI.pdf" target="_blank" rel="noopener">Researchers downloaded 100k+ web pages and built a giant database</a>. Users can search design examples in the database by CSS properties, e.g., font size, widget position, and global page layout. Here are some search results:</p>

<ul>
  <li>designs with font size &gt; 100px</li>
</ul>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/state-of-the-art/fig1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/state-of-the-art/fig1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/state-of-the-art/fig1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/state-of-the-art/fig1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<ul>
  <li>designs with the search bar in the middle of a page:</li>
</ul>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/state-of-the-art/fig2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/state-of-the-art/fig2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/state-of-the-art/fig2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/state-of-the-art/fig2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<ul>
  <li>designs for global layout given a layout query (leftmost)</li>
</ul>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/state-of-the-art/fig3-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/state-of-the-art/fig3-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/state-of-the-art/fig3-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/state-of-the-art/fig3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h3 id="what-attendees-said">What attendees said:</h3>
<p>One of the attendees expressed concerns about searching by CSS properties, as the same widget could be named differently across webpages (e.g., carousel vs. slider).</p>

<h3 id="12-search-by-descriptive-words">1.2 Search by descriptive words</h3>
<p><a href="https://dritchie.github.io/pdf/dtour.pdf" target="_blank" rel="noopener">Researchers have also mapped descriptive words with Document Object Model (DOM) and CSS properties</a>. Users can type descriptive words directly; after translation, the search engine will retrieve results based on the definition of CSS properties.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/state-of-the-art/fig4-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/state-of-the-art/fig4-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/state-of-the-art/fig4-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/state-of-the-art/fig4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h3 id="what-attendees-said-1">What attendees said:</h3>
<p>Designers in attendance shared keywords they usually use:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>minimal/minimalism/minimalistic

neat/clean/simple/easy

colorful/bright

cheerful/fun

dark
</code></pre></div></div>

<h3 id="13-retrieve-images-by-visual-properties">1.3 Retrieve images by visual properties</h3>
<p>In the pre-deep-learning era, retrieving similar images by visual properties achieved reasonably good results. For example, by combining color histogram and Histogram of Oriented Gradients (HOG), <a href="https://arxiv.org/pdf/1505.01214.pdf" target="_blank" rel="noopener">researchers have found stylistically similar infographics</a>.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/state-of-the-art/fig5-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/state-of-the-art/fig5-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/state-of-the-art/fig5-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/state-of-the-art/fig5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h3 id="14-retrieve-images-by-image-embeddings">1.4 Retrieve images by image embeddings</h3>
<p>Neural network-based methods allow users to search for similar UIs by high-fidelity screenshots, wireframes, sketches, and even partial sketches.</p>

<p><a href="https://arxiv.org/abs/2102.05216" target="_blank" rel="noopener">Search similar UI designs by high-fidelity screenshots</a>.</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/state-of-the-art/fig6-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/state-of-the-art/fig6-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/state-of-the-art/fig6-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/state-of-the-art/fig6.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p><a href="https://arxiv.org/abs/2102.05216" target="_blank" rel="noopener">Search similar UI designs by wireframes</a>.</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/state-of-the-art/fig7-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/state-of-the-art/fig7-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/state-of-the-art/fig7-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/state-of-the-art/fig7.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p><a href="https://dl.acm.org/doi/fullHtml/10.1145/3290605.3300334" target="_blank" rel="noopener">Search similar UI designs by sketches</a></p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/state-of-the-art/fig8-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/state-of-the-art/fig8-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/state-of-the-art/fig8-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/state-of-the-art/fig8.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>Although the above methods take different inputs, they adopt the same approach: learning image embeddings (a low-dimensional vector representation) and performing nearest neighbor searches.</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/state-of-the-art/fig9-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/state-of-the-art/fig9-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/state-of-the-art/fig9-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/state-of-the-art/fig9.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
    An illustration of embedding space of UIs adapted from
    <a href="https://dl.acm.org/doi/fullHtml/10.1145/3290605.3300334">Swire: Sketch-based User Interface Retrieval</a>
</div>

<h3 id="15-retrieve-images-by-chatbots">1.5 Retrieve images by chatbots</h3>
<p>Chatbots could help users find design examples in a database via conversations.</p>

<p><a href="https://userinterfaces.aalto.fi/hey_gui/" target="_blank" rel="noopener">An example chatbot that helps users retrieve UI examples</a></p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/state-of-the-art/fig10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/state-of-the-art/fig10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/state-of-the-art/fig10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/state-of-the-art/fig10.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h3 id="what-attendees-said-2">What attendees said:</h3>
<p>23 attendees voted for their preferred methods to find design examples/inspirations (note: multiple selections were allowed; see figure below). Searching by wireframes was the most popular (65%), followed by high-fidelity screenshots (57%), while searching by chatbot (26%) and stylistic keywords (22%) were less popular. This is likely because designers found keyword-based search results to be too broad.</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/state-of-the-art/fig12-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/state-of-the-art/fig12-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/state-of-the-art/fig12-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/state-of-the-art/fig12.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h2 id="part-2-generate-layout-automatically">Part 2: Generate layout automatically</h2>
<p>Motivation: When seeking inspiration, most designers (22 out of 24) collect layout-related examples. If machine learning could automate the layout creation process, that would be a great time saver. Below I discuss options for doing this.</p>

<h3 id="21-generate-layout-based-on-constraints">2.1 Generate layout based on constraints</h3>
<p><a href="https://www.youtube.com/watch?v=y8pTC6FEsKc" target="_blank" rel="noopener">Scout</a> generates feasible layouts given constraints specified by users. Constraints could be non-overlapping, minimal sizes, alignment, visual hierarchy (paddings within groups), emphasis (canâ€™t decrease the size of an important element), etc.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/state-of-the-art/fig13-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/state-of-the-art/fig13-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/state-of-the-art/fig13-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/state-of-the-art/fig13.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h3 id="what-attendees-said-3">What attendees said:</h3>
<p>Designers shared typical constraints they encountered, including:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>information hierarchy

branding requirements

headers and footers
</code></pre></div></div>

<h2 id="22-generate-layout-based-on-energies-design-guidelines">2.2 Generate layout based on â€œenergiesâ€ (design guidelines)</h2>
<p><a href="http://www.dgp.toronto.edu/~donovan/design/index.html" target="_blank" rel="noopener">DesignScape</a> generates layouts by minimizing design guideline violations (â€œenergiesâ€). Design guideline violations include alignment violations, balance violations, overlap, etc.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/state-of-the-art/fig14-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/state-of-the-art/fig14-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/state-of-the-art/fig14-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/state-of-the-art/fig14.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
    Left: random layout; right: after minimizing design guideline violations.
</div>

<p><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/08/a33-yang.pdf" target="_blank" rel="noopener">Automatic Generation of Visual-Textual Presentation Layout</a></p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/state-of-the-art/fig15-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/state-of-the-art/fig15-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/state-of-the-art/fig15-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/state-of-the-art/fig15.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h3 id="23-generate-layout-based-on-keywordscategories">2.3 Generate layout based on keywords/categories</h3>
<p>Researchers have trained Generative Adversarial Networks (GAN) to generate magazine layouts based on images, text, and magazine categories. Below are different magazine layouts generated for the same image but with different keywords.</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/state-of-the-art/fig16-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/state-of-the-art/fig16-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/state-of-the-art/fig16-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/state-of-the-art/fig16.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
    (Content-aware generative modeling of graphic design layouts)[https://xtqiao.com/projects/content_aware_layout/]{:target="_blank" rel="noopener"}
</div>

<h3 id="24-generate-layout-based-on-spatial-relations">2.4 Generate layout based on spatial relations</h3>
<p>Researchers have also <a href="https://arxiv.org/abs/1912.09421" target="_blank" rel="noopener">modeled spatial relations among objects like graphs and trained Graph Convolutional Networks (GCN)</a> to generate designs satisfying such relations.</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/state-of-the-art/fig17-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/state-of-the-art/fig17-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/state-of-the-art/fig17-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/state-of-the-art/fig17.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h3 id="25-generate-layout-with-background-and-embellishments">2.5 Generate layout with background and embellishments</h3>
<p>Most ML methods generate layouts using only given assets. A prototype called Vinci could suggest 
<a href="https://www.youtube.com/watch?v=XNA0diI-LXg" target="_blank" rel="noopener">background and embellishments</a>. Vinci views the design process as linear design sequences and leverages a sequence-to-sequence Variational Autoencoder (VAE) to select background and embellishments.</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/state-of-the-art/fig18-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/state-of-the-art/fig18-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/state-of-the-art/fig18-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/state-of-the-art/fig18.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h3 id="what-attendees-said-4">What attendees said:</h3>
<p>21 attendees voted for their favorite layout generation methods. <a href="https://www.youtube.com/watch?v=XNA0diI-LXg" target="_blank" rel="noopener">Vinci</a> garnered the most votes (62%), followed by spatial relations (48%). Full results are below:</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/state-of-the-art/fig20-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/state-of-the-art/fig20-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/state-of-the-art/fig20-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/state-of-the-art/fig20.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h2 id="takeaways">Takeaways:</h2>
<ul>
  <li>Some important ML techniques donâ€™t lead to popular features, e.g., search design examples by chatbots (note: the talk was given before ChatGPT released).</li>
  <li>The same ML component could lead to features with varied popularity, e.g., searching by wireframe is more popular than searching by high-fidelity/sketches.</li>
  <li>Designers are eager to adopt such techniques! One designer even asked whether itâ€™s possible to download one of the â€œtools.â€ Unfortunately, itâ€™s only research code: no server, no front-end ğŸ¤·â€â™€</li>
</ul>

<h2 id="further-reading">Further reading:</h2>
<p>Some papers donâ€™t fall into the above two topics but are still interesting to read:</p>

<ul>
  <li><a href="https://arxiv.org/abs/2110.07775" target="_blank" rel="noopener">Create UI by natural language descriptions</a></li>
  <li><a href="https://arxiv.org/pdf/2001.05308.pdf" target="_blank" rel="noopener">UI auto-completion</a></li>
  <li><a href="https://link.springer.com/chapter/10.1007/978-3-030-82681-9_8" target="_blank" rel="noopener">How Material Design is used on mobile UIs</a></li>
  <li><a href="https://faculty.washington.edu/ajko/papers/Swearngin2018Rewire.pdf" target="_blank" rel="noopener">Extract wireframes from screenshots</a></li>
</ul>]]></content><author><name></name></author><category term="machine-learning" /><summary type="html"><![CDATA[a 1-hour tutorial talk given at the HCIL symposium 2022]]></summary></entry><entry><title type="html">ScholarInsight: a Chrome browser extension for visualizing Google Scholar profiles</title><link href="https://tracyyxchen.github.io/blog/2020/scholarInsight/" rel="alternate" type="text/html" title="ScholarInsight: a Chrome browser extension for visualizing Google Scholar profiles" /><published>2020-12-21T00:00:00+00:00</published><updated>2020-12-21T00:00:00+00:00</updated><id>https://tracyyxchen.github.io/blog/2020/scholarInsight</id><content type="html" xml:base="https://tracyyxchen.github.io/blog/2020/scholarInsight/"><![CDATA[<p>ScholarInsight was my course project for <a href="https://sites.umiacs.umd.edu/elm/teaching/inst-760-data-visualization/" target="_blank" rel="noopener">INST760</a> in 2020 Fall. I deployed it to Google Web Store for a while but took it down after a Chrome update broke my code.</p>
<ul>
  <li><a href="https://youtu.be/7-C5sBFzqNc" target="_blank" rel="noopener">video demo</a></li>
  <li><a href="https://github.com/TracyYXChen/ScholarInsight" target="_blank" rel="noopener">GitHub repo</a></li>
</ul>

<h2 id="limitations-of-google-scholar">Limitations of Google Scholar</h2>
<p>Despite its popularity, Google Scholar provides limited user interactions on scholarsâ€™ profile pages: papers can only be sorted by year or citations. While itâ€™s easy for people to identify the most cited or recent papers, itâ€™s hard to spot papers published a few years ago but gradually gained more attraction. Besides, Google Scholar treats all citations from papers of different authorships equally.</p>

<h2 id="scholarinsight">ScholarInsight</h2>
<p>At first, I thought about creating interactive data visualizations based on offline datasets. However, citations change every day, so I decided to build an online tool to visualize the latest citation data. Therefore, I created a Chrome web browser extension called ScholarInsight. As a Chrome extension, it can easily be applied to any Google Scholar profile.</p>

<p>Here is what ScholarInsight looks like:</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/scholarInsight/fig1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/scholarInsight/fig1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/scholarInsight/fig1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/scholarInsight/fig1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>ScholarInsight creates a side panel for any Google Scholar profile page. The primary view is a scatter plot, of which the x-axis is the published year and the y-axis is the number of citations. Each colored dot represents a paper: orange denotes first-author papers, blue denotes last-author papers, and grey denotes other papers. ScholarInsight also calculates the authorâ€™s median citation for each year and connects them via a dashed line.</p>

<p>ScholarInsight provides several ways for users to interact with:</p>

<ul>
  <li><strong>Filter</strong>: users can filter papers by checking/unchecking paper type boxes; currently ScholarInsight supports first-author, last-author, and other types.</li>
  <li><strong>Scale-transform</strong>: users can choose linear or log-scale for citations</li>
  <li><strong>Zoom/pan</strong>: users can either click the zoom buttons or use the mouse wheel to zoom; users can also pan the visualization by dragging it.</li>
  <li><strong>Tooltips</strong>: users can hover on the dot, and click it to show detailed information about that paper.</li>
</ul>

<h2 id="insights-uncovered-by-scholarinsight">Insights uncovered by ScholarInsight</h2>

<ul>
  <li>
    <h3 id="a-best-paper">a best paper</h3>
  </li>
</ul>

<p>Letâ€™s zoom in and take a look at the recent first-author publications of this researcher:</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/scholarInsight/fig2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/scholarInsight/fig2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/scholarInsight/fig2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/scholarInsight/fig2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>a paper published in 2018 that is way above the median citation line (the dot becomes larger because we hovered on it), and if we click it, itâ€™s â€œData Illustrator.â€ The paper won the best paper award of the CHI (Computer-Human Interaction) conference. It is also a popular data visualization authoring tool: Data Illustrator has its own Twitter account and has attracted 1200+ followers.</p>

<p>Sometimes the linear scale doesnâ€™t work well; for example, letâ€™s look at this researcherâ€™s profile:</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/scholarInsight/fig3-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/scholarInsight/fig3-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/scholarInsight/fig3-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/scholarInsight/fig3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>At first glimpse, something went wrong. Why is there only one dot? Oh wait, thatâ€™s because one paper has more than 17k citations; thus, all papers are pushed to the bottom. In this case, we can switch to the log-scale.</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/scholarInsight/fig4-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/scholarInsight/fig4-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/scholarInsight/fig4-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/scholarInsight/fig4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<ul>
  <li>
    <h3 id="career-transitions">career transitions</h3>
    <p>If we only look at the first-author and last-author papers, we can see a rough boundary: this researcher started to have last-author papers after 2009. What happened? Well, yes, he became a professor around that time. For productive authors, more than one paper published in the same year may have the same number of citations, so ScholarInsight also jittered the data to avoid visual clutter.</p>
  </li>
</ul>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/scholarInsight/fig5-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/scholarInsight/fig5-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/scholarInsight/fig5-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/scholarInsight/fig5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>Above is the profile of Andrew Ng, a renowned machine-learning researcher. Why has he had many first-author papers in recent years when heâ€™s already a tenured professor? It turned out that he spent more time introducing artificial intelligence to a broader audience, so he started to publish less technical papers, like this one: Artificial intelligence is the new electricity.</p>

<h2 id="user-studies">User studies</h2>
<p>I conducted informal user study sessions of three female computer science Ph.D. students. In their research fields, authorship order is contribution-based. They applied ScholarInsight to one of the researchers they know and shared their findings. For example, they quickly identified the most-cited first/last-author papers. One participant disappointedly found that although the researcher she chose has hundreds of citations, none are first or last-author papers.</p>

<p>In the future, ScholarInsight could help novice researchers or students who apply for graduate schools.</p>]]></content><author><name></name></author><category term="data-visualization" /><summary type="html"><![CDATA[a course project for data visualization]]></summary></entry></feed>